{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SarcasmDetection3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCBGIaYK2UAy",
        "colab_type": "text"
      },
      "source": [
        "# **Load Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBm4zxgd2ZeY",
        "colab_type": "text"
      },
      "source": [
        "# **REMOVE UNWANTED SYMBOLS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DnCUaaAnrRT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "43a69709-20da-47a4-a864-357c6dfe16d0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json(\"Sarcasm_Headlines_Dataset.json\", lines=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_link</th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
              "      <td>former versace store clerk sues over secret 'b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
              "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
              "      <td>mom starting to fear son's web series closest ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
              "      <td>boehner just wants wife to listen, not come up...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
              "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        article_link  ... is_sarcastic\n",
              "0  https://www.huffingtonpost.com/entry/versace-b...  ...            0\n",
              "1  https://www.huffingtonpost.com/entry/roseanne-...  ...            0\n",
              "2  https://local.theonion.com/mom-starting-to-fea...  ...            1\n",
              "3  https://politics.theonion.com/boehner-just-wan...  ...            1\n",
              "4  https://www.huffingtonpost.com/entry/jk-rowlin...  ...            0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjDIHubdrlet",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "9b8c798b-5fda-450f-cd9b-72c29ab276c2"
      },
      "source": [
        "import string\n",
        "from string import digits, punctuation\n",
        "\n",
        "hl_cleansed = []\n",
        "for hl in df['headline']:\n",
        "#     Remove punctuations\n",
        "    clean = hl.translate(str.maketrans('', '', punctuation))\n",
        "#     Remove digits/numbers\n",
        "    clean = clean.translate(str.maketrans('', '', digits))\n",
        "    hl_cleansed.append(clean)\n",
        "    \n",
        "# View comparison\n",
        "print('Original texts :')\n",
        "print(df['headline'][37])\n",
        "print('\\nAfter cleansed :')\n",
        "print(hl_cleansed[37])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original texts :\n",
            "'moana' sails straight to the top of the box office with massive $81.1 million opening\n",
            "\n",
            "After cleansed :\n",
            "moana sails straight to the top of the box office with massive  million opening\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-l-zH332fZp",
        "colab_type": "text"
      },
      "source": [
        "# **TOKENS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlAkPBDGrr1q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "363ef786-c5fe-40c9-d0b0-8f3876a25886"
      },
      "source": [
        "# Tokenization process\n",
        "hl_tokens = []\n",
        "for hl in hl_cleansed:\n",
        "    hl_tokens.append(hl.split())\n",
        "\n",
        "# View Comparison\n",
        "index = 100\n",
        "print('Before tokenization :')\n",
        "print(hl_cleansed[index])\n",
        "print('\\nAfter tokenization :')\n",
        "print(hl_tokens[index])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before tokenization :\n",
            "demi lovato drops emotional nightingale music vid\n",
            "\n",
            "After tokenization :\n",
            "['demi', 'lovato', 'drops', 'emotional', 'nightingale', 'music', 'vid']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94NdPY0_2iQw",
        "colab_type": "text"
      },
      "source": [
        "# **LEMMATIZER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue8jBV-mruxz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "f7e3c484-e87c-46eb-896e-859c605f6401"
      },
      "source": [
        "# Lemmatize with appropriate POS Tag\n",
        "# Credit : www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Init Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "hl_lemmatized = []\n",
        "for tokens in hl_tokens:\n",
        "    lemm = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens]\n",
        "    hl_lemmatized.append(lemm)\n",
        "    \n",
        "# Example comparison\n",
        "word_1 = ['skyrim','dragons', 'are', 'having', 'parties']\n",
        "word_2 = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_1]\n",
        "print('Before lemmatization :\\t',word_1)\n",
        "print('After lemmatization :\\t',word_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Before lemmatization :\t ['skyrim', 'dragons', 'are', 'having', 'parties']\n",
            "After lemmatization :\t ['skyrim', 'dragon', 'be', 'have', 'party']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlbqbybf2wI4",
        "colab_type": "text"
      },
      "source": [
        "# **TEXT TO VECTOR**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evBVAyOErzfI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "c95e2c06-afac-41a2-ef06-dd551e4da37d"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Vectorize and convert text into sequences\n",
        "max_features = 2000\n",
        "max_token = len(max(hl_lemmatized))\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(hl_lemmatized)\n",
        "sequences = tokenizer.texts_to_sequences(hl_lemmatized)\n",
        "X = pad_sequences(sequences, maxlen=max_token)\n",
        "\n",
        "index = 10\n",
        "print('Before :')\n",
        "print(hl_lemmatized[index],'\\n')\n",
        "print('After sequences convertion :')\n",
        "print(sequences[index],'\\n')\n",
        "print('After padding :')\n",
        "print(X[index])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Before :\n",
            "['airline', 'passenger', 'tackle', 'man', 'who', 'rush', 'cockpit', 'in', 'bomb', 'threat'] \n",
            "\n",
            "After sequences convertion :\n",
            "[840, 1011, 1987, 13, 36, 1241, 4, 1689, 629] \n",
            "\n",
            "After padding :\n",
            "[   0    0    0    0    0  840 1011 1987   13   36 1241    4 1689  629]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apUljVnx23WU",
        "colab_type": "text"
      },
      "source": [
        "# **MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R1AuHJWspEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Y = df['is_sarcastic'].values\n",
        "Y = np.vstack(Y)\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcBhQiW7ss3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "\n",
        "embed_dim = 64\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embed_dim,input_length = max_token))\n",
        "model.add(LSTM(96, dropout=0.2, recurrent_dropout=0.2, activation='relu'))\n",
        "# model.add(Dense(128))\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV_gVT0W28lG",
        "colab_type": "text"
      },
      "source": [
        "# **TRAIN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12952c_BswgK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "95eb3cf6-78ef-4730-d519-9471d79d794d"
      },
      "source": [
        "epoch = 10\n",
        "batch_size = 128\n",
        "model.fit(X_train, Y_train, epochs = epoch, batch_size=batch_size, verbose = 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " - 6s - loss: 0.5379 - accuracy: 0.7171\n",
            "Epoch 2/10\n",
            " - 5s - loss: 0.3767 - accuracy: 0.8401\n",
            "Epoch 3/10\n",
            " - 5s - loss: 0.3372 - accuracy: 0.8532\n",
            "Epoch 4/10\n",
            " - 5s - loss: 0.3182 - accuracy: 0.8638\n",
            "Epoch 5/10\n",
            " - 5s - loss: 0.3089 - accuracy: 0.8675\n",
            "Epoch 6/10\n",
            " - 5s - loss: 0.2924 - accuracy: 0.8757\n",
            "Epoch 7/10\n",
            " - 5s - loss: 0.2818 - accuracy: 0.8792\n",
            "Epoch 8/10\n",
            " - 5s - loss: 0.2678 - accuracy: 0.8851\n",
            "Epoch 9/10\n",
            " - 5s - loss: 0.2608 - accuracy: 0.8861\n",
            "Epoch 10/10\n",
            " - 5s - loss: 0.2491 - accuracy: 0.8946\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f68e53b8748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgYoqONGbeDA",
        "colab_type": "text"
      },
      "source": [
        "# **TEST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfOGsySBs2f-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "5af7c36a-204c-48d7-9b53-4f105db059f0"
      },
      "source": [
        "loss, acc = model.evaluate(X_test, Y_test, verbose=2)\n",
        "print(\"Overall scores\")\n",
        "print(\"Loss\\t\\t: \", round(loss, 3))\n",
        "print(\"Accuracy\\t: \", round(acc, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overall scores\n",
            "Loss\t\t:  0.409\n",
            "Accuracy\t:  0.834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCWaeT3G3CBU",
        "colab_type": "text"
      },
      "source": [
        "# ACCURACY SCORE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixy9Nu44s53g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
        "for x in range(len(X_test)):\n",
        "    \n",
        "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
        "   \n",
        "    if np.around(result) == np.around(Y_test[x]):\n",
        "        if np.around(Y_test[x]) == 0:\n",
        "            neg_correct += 1\n",
        "        else:\n",
        "            pos_correct += 1\n",
        "       \n",
        "    if np.around(Y_test[x]) == 0:\n",
        "        neg_cnt += 1\n",
        "    else:\n",
        "        pos_cnt += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixd4BKYFs8LB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1e8825d4-3869-4260-8f43-efe30ad03448"
      },
      "source": [
        "print(\"Sarcasm accuracy\\t: \", round(pos_correct/pos_cnt*100, 3),\"%\")\n",
        "print(\"Non-sarcasm accuracy\\t: \", round(neg_correct/neg_cnt*100, 3),\"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sarcasm accuracy\t:  84.477 %\n",
            "Non-sarcasm accuracy\t:  82.606 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ekoh3tG2CI1",
        "colab_type": "text"
      },
      "source": [
        "# **INPUT PUNCTUATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2R88uvXp2UX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "8ce6b2d2-093a-4bb0-da7b-4d87418f7f57"
      },
      "source": [
        "\"\"\"input2 = [\"world crowd applauds for 'dolphin' playfully 98979801 spraying blood from blowhole\"]  #sarcastic\n",
        "input3 = [\"former versace store clerk sues over secret 'black code' for minority shoppers\"]      #not\n",
        "\"\"\"\n",
        "inputt=[\"My name is Akilesh\"]\n",
        "\n",
        "input2=[\"It’s okay if you don’t like me. Not everyone has good taste.\"]                          #sarcastic\n",
        "input_cleansed=[]\n",
        "\n",
        "\n",
        "#Remove punctuations\n",
        "clean = inputt[0].translate(str.maketrans('', '', punctuation))\n",
        "#Remove digits/numbers\n",
        "clean = clean.translate(str.maketrans('', '', digits))\n",
        "input_cleansed.append(clean)\n",
        "print(input_cleansed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['My name is Akilesh']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i1VaLP_3Mkp",
        "colab_type": "text"
      },
      "source": [
        "# **INPUT TOKEN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rara46Y3Qz8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "c38c0c36-eb88-442a-fb75-bfb80b9ca5d9"
      },
      "source": [
        "input_tokens = []\n",
        "for input in input_cleansed:\n",
        "    input_tokens.append(input.split())\n",
        "\n",
        "# View Comparison\n",
        "index = 0\n",
        "print('Before tokenization :')\n",
        "print(input_cleansed[index])\n",
        "print('\\nAfter tokenization :')\n",
        "print(input_tokens[index])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before tokenization :\n",
            "My name is Akilesh\n",
            "\n",
            "After tokenization :\n",
            "['My', 'name', 'is', 'Akilesh']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho-3QGltJ1Tj",
        "colab_type": "text"
      },
      "source": [
        "# INPUT LEMMA "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9GsynUtGS7g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "68e7c92d-26cf-4c2f-c93d-ce1085bea908"
      },
      "source": [
        "# Init Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "input_lemmatized = []\n",
        "for tokens in input_tokens:\n",
        "    lemm = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens]\n",
        "    input_lemmatized.append(lemm)\n",
        "\n",
        "print(input_lemmatized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['My', 'name', 'be', 'Akilesh']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBi_mZ9XKakC",
        "colab_type": "text"
      },
      "source": [
        "# **INPUT VECTOR**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_cfpkF9Hq8O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9bd3145b-7baf-4d16-c6da-04f807e6953e"
      },
      "source": [
        "# Vectorize and convert text into sequences\n",
        "\n",
        "max_features = 2000\n",
        "max_token = len(max(hl_lemmatized))\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(hl_lemmatized)\n",
        "sequences = tokenizer.texts_to_sequences(input_lemmatized)\n",
        "X = pad_sequences(sequences, maxlen=max_token)\n",
        "\n",
        "\n",
        "print(sequences)\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[85, 178, 5]]\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0  85 178   5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPwm8U_gZoSC",
        "colab_type": "text"
      },
      "source": [
        "# **INPUT PREDICTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8PFlqJqZqtA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "66271ef0-597c-476d-f1fe-e1f9b3915ff3"
      },
      "source": [
        "res=model.predict(X)\n",
        "print(res)\n",
        "if res>0.5:\n",
        "  print(\"Sarcastic\")\n",
        "else:\n",
        "  print(\"Not Sarcastic\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.10508372]]\n",
            "Not Sarcastic\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}